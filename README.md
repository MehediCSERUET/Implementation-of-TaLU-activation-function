# Implementation-of-TaLU-activation-function
This is a repository for the implementation for TaLU: A Hybrid Activation Function Combining Tanh and Rectified Linear Unit to Enhance Neural Networks paper 
Link of the paper: https://arxiv.org/abs/2305.04402
