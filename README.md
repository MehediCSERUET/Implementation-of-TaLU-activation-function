# Implementation-of-TaLU-activation-function
This is a repository for the implementation for TaLU: A Hybrid Activation Function Combining Tanh and Rectified Linear Unit to Enhance Neural Networks paper
